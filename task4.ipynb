{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Initialize PySpark"
      ],
      "metadata": {
        "id": "DARiLF3FOPqw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kaiQCsOENqmP",
        "outputId": "dc9dac85-fc69-4fde-afa0-f9f8d11e5532"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.5.3\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize a Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Retail Sales Sentiment Analysis\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Check Spark session\n",
        "print(spark.version)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading Dataset"
      ],
      "metadata": {
        "id": "kajIzy7HtRop"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "data_path = \"/content/amazon_reviews.csv\"\n",
        "df = spark.read.csv(data_path, header=True, inferSchema=True)\n",
        "\n",
        "# Show the first few rows\n",
        "df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHcrUwGFOozo",
        "outputId": "14a8af8b-e585-458b-80fa-562f89889bd9"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------------+-------+--------------------+----------+--------+-----------+----------+----------+------------------+--------------------+------------------+\n",
            "|_c0|reviewerName|overall|          reviewText|reviewTime|day_diff|helpful_yes|helpful_no|total_vote|score_pos_neg_diff|score_average_rating|wilson_lower_bound|\n",
            "+---+------------+-------+--------------------+----------+--------+-----------+----------+----------+------------------+--------------------+------------------+\n",
            "|  0|        NULL|    4.0|          No issues.|2014-07-23|     138|          0|         0|         0|                 0|                 0.0|               0.0|\n",
            "|  1|        0mie|    5.0|Purchased this fo...|2013-10-25|     409|          0|         0|         0|                 0|                 0.0|               0.0|\n",
            "|  2|         1K3|    4.0|it works as expec...|2012-12-23|     715|          0|         0|         0|                 0|                 0.0|               0.0|\n",
            "|  3|         1m2|    5.0|This think has wo...|2013-11-21|     382|          0|         0|         0|                 0|                 0.0|               0.0|\n",
            "|  4|2&amp;1/2Men|    5.0|Bought it with Re...|2013-07-13|     513|          0|         0|         0|                 0|                 0.0|               0.0|\n",
            "+---+------------+-------+--------------------+----------+--------+-----------+----------+----------+------------------+--------------------+------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analyzing Missing Values"
      ],
      "metadata": {
        "id": "qHJRAR-GtWjE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Count total rows\n",
        "total_rows = df.count()\n",
        "\n",
        "# Count missing values in each column\n",
        "missing_counts = {col_name: df.filter(col(col_name).isNull()).count() for col_name in df.columns}\n",
        "\n",
        "# Print missing values summary\n",
        "for col_name, count in missing_counts.items():\n",
        "    print(f\"Column: {col_name}, Missing Values: {count}, Percentage: {(count / total_rows) * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5I462EVbRjkV",
        "outputId": "cdfa0fed-b134-4480-bc4d-f9570ef5ca46"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Column: _c0, Missing Values: 0, Percentage: 0.00%\n",
            "Column: reviewerName, Missing Values: 1, Percentage: 0.02%\n",
            "Column: overall, Missing Values: 0, Percentage: 0.00%\n",
            "Column: reviewText, Missing Values: 1, Percentage: 0.02%\n",
            "Column: reviewTime, Missing Values: 1, Percentage: 0.02%\n",
            "Column: day_diff, Missing Values: 2, Percentage: 0.04%\n",
            "Column: helpful_yes, Missing Values: 1, Percentage: 0.02%\n",
            "Column: helpful_no, Missing Values: 0, Percentage: 0.00%\n",
            "Column: total_vote, Missing Values: 1, Percentage: 0.02%\n",
            "Column: score_pos_neg_diff, Missing Values: 1, Percentage: 0.02%\n",
            "Column: score_average_rating, Missing Values: 0, Percentage: 0.00%\n",
            "Column: wilson_lower_bound, Missing Values: 1, Percentage: 0.02%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removing row in the dataset where reviewText is null and Filling other missing values"
      ],
      "metadata": {
        "id": "PvBJlAUCte33"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.filter(df[\"reviewText\"].isNotNull())"
      ],
      "metadata": {
        "id": "MCF6KNfuRtWE"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill missing values for columns not directly used\n",
        "from pyspark.sql import functions as F\n",
        "df = df.fillna({\n",
        "    \"reviewerName\": \"Anonymous\",\n",
        "    \"reviewTime\": \"Unknown\",\n",
        "    \"day_diff\": -1,\n",
        "    \"helpful_yes\": 0,\n",
        "    \"helpful_no\": 0,\n",
        "    \"total_vote\": 0,\n",
        "    \"score_pos_neg_diff\": 0,\n",
        "    \"wilson_lower_bound\": 0\n",
        "})\n",
        "df.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in df.columns]).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQquSTAER4Hp",
        "outputId": "18966318-29b1-4fa6-9393-24acd3875c4d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------------+-------+----------+----------+--------+-----------+----------+----------+------------------+--------------------+------------------+\n",
            "|_c0|reviewerName|overall|reviewText|reviewTime|day_diff|helpful_yes|helpful_no|total_vote|score_pos_neg_diff|score_average_rating|wilson_lower_bound|\n",
            "+---+------------+-------+----------+----------+--------+-----------+----------+----------+------------------+--------------------+------------------+\n",
            "|  0|           0|      0|         0|         0|       0|          0|         0|         0|                 0|                   0|                 0|\n",
            "+---+------------+-------+----------+----------+--------+-----------+----------+----------+------------------+--------------------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover, NGram, HashingTF, IDF\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.sql.functions import when, avg\n",
        "\n",
        "# Step 1: Label Creation (Based on 'overall' Ratings)\n",
        "# Positive Sentiment (1): Ratings 4 and 5\n",
        "# Negative Sentiment (0): Ratings 1 and 2\n",
        "df = df.withColumn(\"label\", when(col(\"overall\") >= 4, 1).when(col(\"overall\") <= 2, 0))\n",
        "df = df.filter(df[\"label\"].isNotNull())  # Remove neutral ratings\n",
        "\n",
        "# Step 2: Text Preprocessing\n",
        "tokenizer = Tokenizer(inputCol=\"reviewText\", outputCol=\"words\")\n",
        "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
        "ngram = NGram(n=2, inputCol=\"filtered\", outputCol=\"ngrams\")\n",
        "\n",
        "# Step 3: Feature Engineering\n",
        "hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=10000)\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"finalFeatures\")\n",
        "\n",
        "# Step 4: Logistic Regression Model\n",
        "lr = LogisticRegression(featuresCol=\"finalFeatures\", labelCol=\"label\")\n",
        "\n",
        "# Step 5: Build and Train Pipeline\n",
        "pipeline = Pipeline(stages=[tokenizer, remover, ngram, hashingTF, idf, lr])\n",
        "\n",
        "# Split data into training and testing\n",
        "(trainingData, testData) = df.randomSplit([0.7, 0.3], seed=1234)\n",
        "\n",
        "# Train the model\n",
        "model = pipeline.fit(trainingData)\n",
        "\n",
        "# Step 6: Predictions\n",
        "predictions = model.transform(testData)\n",
        "\n",
        "# Step 7: Evaluate the Model\n",
        "evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
        "auc = evaluator.evaluate(predictions)\n",
        "print(f\"Area Under ROC: {auc}\")\n",
        "\n",
        "# Step 8: Aggregate Sentiment Scores\n",
        "product_sentiment = predictions.groupBy(\"overall\").agg(avg(\"prediction\").alias(\"avg_sentiment\"))\n",
        "product_sentiment.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-SwO9U0S3Ut",
        "outputId": "2bd9c4ac-0017-403b-c670-593beeda7c71"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Area Under ROC: 0.8782051282051285\n",
            "+-------+------------------+\n",
            "|overall|     avg_sentiment|\n",
            "+-------+------------------+\n",
            "|    1.0|0.3235294117647059|\n",
            "|    5.0|0.9220665499124343|\n",
            "|    4.0|0.8766233766233766|\n",
            "|    2.0|0.5652173913043478|\n",
            "+-------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_path = \"/content/output\"\n",
        "product_sentiment.write.csv(f\"{output_path}/product_sentiments.csv\", header=True, mode=\"overwrite\")\n",
        "\n",
        "# Save Evaluation Results\n",
        "with open(f\"{output_path}/evaluation_results.txt\", \"w\") as file:\n",
        "    file.write(f\"Area Under ROC: {auc}\\n\")\n",
        "\n",
        "# Stop Spark session\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "HluwLTSGzcxB"
      },
      "execution_count": 23,
      "outputs": []
    }
  ]
}